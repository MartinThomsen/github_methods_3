---
title: "Portfolio Assignment 3"
author: 'Martin Thomsen'
date: "05-12-2021"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>


# Exercises and objectives

1) Load the magnetoencephalographic recordings and do some initial plots to understand the data  
2) Do logistic regression to classify pairs of PAS-ratings  
3) Do a Support Vector Machine Classification on all four PAS-ratings  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This is Assignment 3 and will be part of your final portfolio   

# EXERCISE 1 - Load the magnetoencephalographic recordings and do some initial plots to understand the data  

The files `megmag_data.npy` and `pas_vector.npy` can be downloaded here (http://laumollerandersen.org/data_methods_3/megmag_data.npy) and here (http://laumollerandersen.org/data_methods_3/pas_vector.npy)   

1) Load `megmag_data.npy` and call it `data` using `np.load`. You can use `join`, which can be imported from `os.path`, to create paths from different string segments
```{python}
# Load data
import numpy as np
megmag = np.load('megmag_data.npy')
pas_vector = np.load('pas_vector.npy')
```
    i. The data is a 3-dimensional array. The first dimension is number of repetitions of a visual stimulus , the second dimension is the number of sensors that record magnetic fields (in Tesla) that stem from neurons activating in the brain, and the third dimension is the number of time samples. How many repetitions, sensors and time samples are there?
```{python}
# Print info
print("Number of repetitions:", megmag.shape[0])
print("Number of sensors:", megmag.shape[1])
print("Number of time samples:", megmag.shape[2])
```
    ii. The time range is from (and including) -200 ms to (and including) 800 ms with a sample recorded every 4 ms. At time 0, the visual stimulus was briefly presented. Create a 1-dimensional array called `times` that represents this.
```{python}
# Create list of times (indicating time in ms relative to onset of stimulus)
times = np.arange(-200, 804, 4).tolist()
```
    iii. Create the sensor covariance matrix $\Sigma_{XX}$: $$\Sigma_{XX} = \frac 1 N \sum_{i=1}^N XX^T$$ $N$ is the number of repetitions and $X$ has $s$ rows and $t$ columns (sensors and time), thus the shape is $X_{s\times t}$. Do the sensors pick up independent signals? (Use `plt.imshow` to plot the sensor covariance matrix)
```{python}
import matplotlib.pyplot as plt

# Initialise matrix with zeroes
cov_matrix = np.zeros(shape = (102,102))
 
# Loop over each repetition and create covariance matrix
for i in range(682):
    cov_matrix += megmag[i,:,:] @ megmag[i,:,:].T
cov_matrix = cov_matrix/682

# Show figure and colour bar
plt.figure
plt.imshow(cov_matrix, cmap = 'hot')
plt.colorbar()
plt.show()
```
Yellow colours indicate a covariance value of 5*10^-23, whereas dark red is -2*10^-23. As the covariance is very small, it can be concluded that the sensors pick up independent signals.
    iv. Make an average over the repetition dimension using `np.mean` - use the `axis` argument. (The resulting array should have two dimensions with time as the first and magnetic field as the second) 
```{python}
# Extract average
megmag_average = megmag.mean(axis = 0)
```
    v. Plot the magnetic field (based on the average) as it evolves over time for each of the sensors (a line for each) (time on the x-axis and magnetic field on the y-axis). Add a horizontal line at $y = 0$ and a vertical line at $x = 0$ using `plt.axvline` and `plt.axhline` 
```{python}
# Plot average magnetic field over time
plt.clf()
plt.plot(times, megmag_average.T)
plt.xlabel("Time (m/S)")
plt.ylabel("Average brain activity (Tesla)")
plt.title("Average brain activity over time")
plt.show()
```
    vi. Find the maximal magnetic field in the average. Then use `np.argmax` and `np.unravel_index` to find the sensor that has the maximal magnetic field.  
```{python}
# Identify and print maximal average magnetic field
peakMag_field = np.max(megmag_average)
print("Maximal average magnetic field:", peakMag_field, "Tesla")

# Extracting information on the index - this returns two values, the first identifies the sensor, the second the time point
index = np.unravel_index(np.argmax(megmag_average), shape = (102, 251))

# Printing information
print("Sensor with the maximal magnetic field is:", index[0])
print("The maximal magnetic field was measured at: ", times[index[1]], "ms")
```
    vii. Plot the magnetic field for each of the repetitions (a line for each) for the sensor that has the maximal magnetic field. Highlight the time point with the maximal magnetic field in the average (as found in 1.1.v) using `plt.axvline`  
```{python}
plt.clf()
plt.figure()

# Plotting data for each repetition, at each time point, for sensor 73
plt.plot(times, megmag[:,73,:].T)
plt.axvline(x = times[112], color =  "black")
plt.xlabel("Time (ms)")
plt.ylabel("Brain activity measured at sensor 73 (Tesla)")
plt.title("All measurements from sensor 73")
plt.show()
```
    viii. Describe in your own words how the response found in the average is represented in the single repetitions. But do make sure to use the concepts _signal_ and _noise_ and comment on any differences on the range of values on the y-axis

The 682 repetitions are shown in the figure above. The maximal average brain activity is seen at 248ms (the black line). Visually separating the signal from the noise is impossible.

2) Now load `pas_vector.npy` (call it `y`). PAS is the same as in Assignment 2, describing the clarity of the subjective experience the subject reported after seeing the briefly presented stimulus  
    i. Which dimension in the `data` array does it have the same length as?
```{python}
pas_vector.shape
```
The pas vector has the same length as the number of repetitions.
    ii. Now make four averages (As in Exercise 1.1.iii), one for each PAS rating, and plot the four time courses (one for each PAS rating) for the sensor found in Exercise ~~1.1.v~~  1.1.vi
```{python}
plt.clf()
# Extracting data on sensor 73
sensor_data = megmag[:,73,:]

# Calculating average brain activity by pas ratings
pas1_avg = np.mean(sensor_data[np.where(pas_vector == 1)], axis = 0)
pas2_avg = np.mean(sensor_data[np.where(pas_vector == 2)], axis = 0)
pas3_avg = np.mean(sensor_data[np.where(pas_vector == 3)], axis = 0)
pas4_avg = np.mean(sensor_data[np.where(pas_vector == 4)], axis = 0)

# Plotting figure containing average brain activity for each pas rating at each time point
plt.figure()
plt.plot(times, pas1_avg)
plt.plot(times, pas2_avg)
plt.plot(times, pas3_avg)
plt.plot(times, pas4_avg)
plt.xlabel("Time (ms)")
plt.ylabel("Average brain activity (Tesla)")
plt.title("Average magnetic field for each pas-rating")
plt.legend(["pas 1", "pas 2", "pas 3", "pas 4"])
plt.show()
```
    iii. Notice that there are two early peaks (measuring visual activity from the brain), one before 200 ms and one around 250 ms. Describe how the amplitudes of responses are related to the four PAS-scores. Does PAS 2 behave differently than expected?
    
The first peak, which shows marked reduced brain activity, shows a relatively smaller brain activity for pas scores 4 and 2, followed by 3 and 1. The next peak shows marked increased brain activity. Here, the pas score 2 shows the highest brain activity and scores 3 and 4 have similar peaks. Below these, pas 1 is found.
Pas 2 would presumably be observed as having peaks more extreme than pas 1, but less than 3 and 4. This is not the case.

# EXERCISE 2 - Do logistic regression to classify pairs of PAS-ratings  

1) Now, we are going to do Logistic Regression with the aim of classifying the PAS-rating given by the subject  
    i. We'll start with a binary problem - create a new array called `data_1_2` that only contains PAS responses 1 and 2. Similarly, create a `y_1_2` for the target vector
```{python}
# Concatenate data - first all repetitions where pas score is 1 and 2
data_1_2 =np.concatenate((megmag[np.where(pas_vector == 1)], megmag[np.where(pas_vector == 2)]), axis = 0)

# Then as a vector of pas scores
y_1_2 = np.concatenate((pas_vector[np.where(pas_vector == 1)], pas_vector[np.where(pas_vector == 2)]), axis = 0)
```
    ii. Scikit-learn expects our observations (`data_1_2`) to be in a 2d-array, which has samples (repetitions) on dimension 1 and features (predictor variables) on dimension 2. Our `data_1_2` is a three-dimensional array. Our strategy will be to collapse our two last dimensions (sensors and time) into one dimension, while keeping the first dimension as it is (repetitions). Use `np.reshape` to create a variable `X_1_2` that fulfils these criteria.
```{python}
X_1_2 = np.reshape(data_1_2, (214, 102*251))
X_1_2.shape
```
    iii. Import the `StandardScaler` and scale `X_1_2`
```{python}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Scaling X_1_2
X_scaled = scaler.fit_transform(X_1_2)
```
    iv. Do a standard `LogisticRegression` - can be imported from `sklearn.linear_model` - make sure there is no `penalty` applied
```{python}
from sklearn.linear_model import LogisticRegression
# Creating a logistic regression with no penalty. The solver is not specified. Random state set to 42, because this is always the answer.
regr = LogisticRegression(random_state = 42, penalty = 'none')
regr.fit(X_scaled, y_1_2)
```
    v. Use the `score` method of `LogisticRegression` to find out how many labels were classified correctly. Are we overfitting? Besides the score, what would make you suspect that we are overfitting?
```{python}
regr.score(X_scaled, y_1_2)
```
The score tells us that every single label has been predicted correctly. Partly, this score is due to there being a considerable difference in average brain activity across pas scores 1 and 2, and partly the lack of a test set. The model is trained on the complete set and will therefore fit to this.    
    vi. Now apply the _L1_ penalty instead - how many of the coefficients (`.coef_`) are non-zero after this?
```{python}
# Solver changed to liblinear, as default solver does not support l1 penalty. Tol set to 0.01 to avoid reaching maximum number of iterations. While this could also be changed to allow for more iterations, tol also sets a limit on desired accuracy
regr_penalty = LogisticRegression(random_state = 42, penalty = 'l1', solver = "liblinear")
regr_penalty.fit(X_scaled, y_1_2)
regr_penalty.score(X_scaled, y_1_2)

non_zero_coef = regr_penalty.coef_[regr_penalty.coef_ != 0]
print("Number of non-zero coefficients: ", len(non_zero_coef))
```
    vii. Create a new reduced $X$ that only includes the non-zero coefficients - show the covariance of the non-zero features (two covariance matrices can be made; $X_{reduced}X_{reduced}^T$ or $X_{reduced}^TX_{reduced}$ (you choose the right one)) . Plot the covariance of the features using `plt.imshow`. Compared to the plot from 1.1.iii, do we see less covariance?
```{python}
non_zero_indices = regr_penalty.coef_.flatten() != 0
X_reduced = X_scaled[:, non_zero_indices]

# Non-zero coefficients covariance matrix
cov_matrix = X_reduced @ np.transpose(X_reduced)

plt.close("all")
plt.figure()
plt.imshow(cov_matrix, cmap = 'hot')
plt.colorbar()
plt.show()
```
    
2) Now, we are going to build better (more predictive) models by using cross-validation as an outcome measure    
    i. Import `cross_val_score` and `StratifiedKFold` from `sklearn.model_selection`  
```{python}
from sklearn.model_selection import cross_val_score, StratifiedKFold
```
    ii. To make sure that our training data sets are not biased to one target (PAS) or the other, create `y_1_2_equal`, which should have an equal number of each target. Create a similar `X_1_2_equal`. The function `equalize_targets_binary` in the code chunk associated with Exercise 2.2.ii can be used. Remember to scale `X_1_2_equal`!
```{python}
# Function to 
def equalize_targets_binary(data, y):
    np.random.seed(7)
    targets = np.unique(y) ## find the number of targets
    if len(targets) > 2:
        raise NameError("can't have more than two targets")
    counts = list()
    indices = list()
    for target in targets:
        counts.append(np.sum(y == target)) ## find the number of each target
        indices.append(np.where(y == target)[0]) ## find their indices
    min_count = np.min(counts)
    # randomly choose trials
    first_choice = np.random.choice(indices[0], size=min_count, replace=False)
    second_choice = np.random.choice(indices[1], size=min_count,replace=False)
    
    # create the new data sets
    new_indices = np.concatenate((first_choice, second_choice))
    new_y = y[new_indices]
    new_data = data[new_indices, :, :]
    
    return new_data, new_y
    
X_1_2_equal, y_1_2_equal = equalize_targets_binary(data_1_2, y_1_2)
X_1_2_equal.shape
X_1_2_equal = X_1_2_equal.reshape(198, -1)
X_1_2_equal = scaler.fit_transform(X_1_2_equal)
```
    iii. Do cross-validation with 5 stratified folds doing standard `LogisticRegression` (See Exercise 2.1.iv)
```{python}
cv = StratifiedKFold()
regr = LogisticRegression()
regr.fit(X_1_2_equal, y_1_2_equal)
scores = cross_val_score(regr, X_1_2_equal, y_1_2_equal, cv = 5)
mean_score = np.mean(scores)
print("Score across five folds: %.3f" % mean_score)
```
    iv. Do L2-regularisation with the following `Cs=  [1e5, 1e1, 1e-5]`. Use the same kind of cross-validation as in Exercise 2.2.iii. In the best-scoring of these models, how many more/fewer predictions are correct (on average)?  
```{python}
Cs = (1e5, 1e1, 1e-5)

for i in range(3):
    temp_regr = LogisticRegression(C = Cs[i], penalty = "l2");
    temp_regr.fit(X_1_2_equal, y_1_2_equal);
    scores[i] = np.mean(cross_val_score(temp_regr, X_1_2_equal, y_1_2_equal, cv = 5));
for i in range(3):
    print("Mean score utilising a Cs of %.5f: %.2f" % (Cs[i], scores[i]))
print("Best model is with a C = 1e-5, which is %.3f better than the initial fit" % (scores[2]-mean_score))
```
    v. Instead of fitting a model on all `n_sensors * n_samples` features, fit  a logistic regression (same kind as in Exercise 2.2.iv (use the `C` that resulted in the best prediction)) for __each__ time sample and use the same cross-validation as in Exercise 2.2.iii. What are the time points where classification is best? Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)
```{python}
X_1_2_equal, y_1_2_equal = equalize_targets_binary(data_1_2, y_1_2)
cv = StratifiedKFold()
regr = LogisticRegression(C=1e-5, penalty="l2", solver = "liblinear")

scores = []

for i in range(251):
    t = scaler.fit_transform(X_1_2_equal[:,:,i]);
    regr.fit(t, y_1_2_equal);
    temp_scores = cross_val_score(regr, t, y_1_2_equal, cv=5);
    scores.append(np.mean(temp_scores))
```

```{python}
X_1_2_equal.shape
```

```{python}
print("Highest classification accuracy is: %.3f" % np.amax(scores))
print("The timepoint at which this occurs is: %.f" % np.argmax(scores))
```

```{python}
plt.figure() 
plt.plot(times, scores)
plt.axvline(times[108], color = "black")
plt.axhline(y = 0.5, color = "green")
plt.xlabel("Time (ms)")
plt.ylabel("Classification accuracy")
plt.title("Classification accuracy at given times")
plt.show()
```
    vi. Now do the same, but with L1 regression - set `C=1e-1` - what are the time points when classification is best? (make a plot)?
```{python}
X_1_2_equal, y_1_2_equal = equalize_targets_binary(data_1_2, y_1_2)
cv = StratifiedKFold()
regr = LogisticRegression(C=1e-1, penalty="l1", solver = "liblinear")
scores = []

for i in range(251):
    t = scaler.fit_transform(X_1_2_equal[:,:,i]);
    regr.fit(t, y_1_2_equal);
    temp_scores = cross_val_score(regr, t, y_1_2_equal, cv=5);
    scores.append(np.mean(temp_scores))
```

```{python}
print("Highest classification accuracy is: %.3f" % np.amax(scores))
print("The timepoint at which this occurs is: %.f" % np.argmax(scores))
```

```{python}
plt.figure() 
plt.plot(times, scores)
plt.axvline(times[111], color = "black")
plt.axhline(y = 0.5, color = "green")
plt.xlabel("Time (ms)")
plt.ylabel("Classification accuracy")
plt.title("Classification accuracy at given times")
plt.show()
```
    vii. Finally, fit the same models as in Exercise 2.2.vi but now for `data_1_4` and `y_1_4` (create a data set and a target vector that only contains PAS responses 1 and 4). What are the time points when classification is best? Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?) 
```{python}
data_1_4 =np.concatenate((megmag[np.where(pas_vector == 1)], megmag[np.where(pas_vector == 4)]), axis = 0)
y_1_4 = np.concatenate((pas_vector[np.where(pas_vector == 1)], pas_vector[np.where(pas_vector == 4)]), axis = 0)

X_1_4_equal, y_1_4_equal = equalize_targets_binary(data_1_4, y_1_4)

cv = StratifiedKFold()
regr = LogisticRegression(C=1e-1, penalty="l1", solver = "liblinear")
scores = []

for i in range(251):
    t = scaler.fit_transform(X_1_4_equal[:,:,i]);
    regr.fit(t, y_1_4_equal);
    temp_scores = cross_val_score(regr, t, y_1_4_equal, cv=5);
    scores.append(np.mean(temp_scores))
```

```{python}
print("Highest classification accuracy is: %.3f" % np.amax(scores))
print("The timepoint at which this occurs is: %.f" % np.argmax(scores))
```

```{python}
plt.figure() 
plt.plot(times, scores)
plt.axvline(times[109], color = "black")
plt.axhline(y = 0.5, color = "green")
plt.xlabel("Time (ms)")
plt.ylabel("Classification accuracy")
plt.title("Classification accuracy at given times")
plt.show()
```
3) Is pairwise classification of subjective experience possible? Any surprises in the classification accuracies, i.e. how does the classification score fore PAS 1 vs 4 compare to the classification score for PAS 1 vs 2?  

The size difference in brain activity is larger for PAS 1 vs. PAS 2, than PAS 1 vs. PAS 4. Intuitively, this makes it more difficult to separate the latter from each. As such, the expectation is a lower classification accuracy for PAS 1 vs. PAS 4. This is also what is observed, with a classification score of 0.642 compared to 0.672. It is worth noting that the classification accuracy is largest around the same time-point, indicating that this is the best time to use for making classifications. 

# EXERCISE 3 - Do a Support Vector Machine Classification on all four PAS-ratings  
1) Do a Support Vector Machine Classification  
    i. First equalize the number of targets using the function associated with each PAS-rating using the function associated with Exercise 3.1.i
```{python}

def equalize_targets(data, y):
    np.random.seed(7)
    targets = np.unique(y)
    counts = list()
    indices = list()
    for target in targets:
        counts.append(np.sum(y == target))
        indices.append(np.where(y == target)[0])
    min_count = np.min(counts)
    first_choice = np.random.choice(indices[0], size=min_count, replace=False)
    second_choice = np.random.choice(indices[1], size=min_count, replace=False)
    third_choice = np.random.choice(indices[2], size=min_count, replace=False)
    fourth_choice = np.random.choice(indices[3], size=min_count, replace=False)
    
    new_indices = np.concatenate((first_choice, second_choice,
                                 third_choice, fourth_choice))
    new_y = y[new_indices]
    new_data = data[new_indices, :, :]
    
    return new_data, new_y
```
```{python}
megmag_equal, y_equal = equalize_targets(megmag, pas_vector)
megmag_equal.shape
y_equal.shape
megmag_equal = megmag_equal.reshape(396, -1)
megmag_equal.shape
```
    ii. Run two classifiers, one with a linear kernel and one with a radial basis (other options should be left at their defaults) - the number of features is the number of sensors multiplied the number of samples. Which one is better predicting the category?
```{python}
from sklearn.svm import SVC
svm_linear = SVC(kernel="linear")
svm_radial = SVC(kernel="rbf")

megmag_scaled = scaler.fit_transform(megmag_equal)

scores_linear = cross_val_score(svm_linear, megmag_scaled, y_equal, cv=cv)
print("The linear classifier score is: %.3f", np.mean(scores_linear))


scores_radial = cross_val_score(svm_radial, megmag_scaled, y_equal, cv=cv)
print("The radial classifier score is: %.3f", np.mean(scores_radial))
```
The better SVM is a radial one, as it outperforms the linear SVM. Both have poor performances however.
    iii. Run the sample-by-sample analysis (similar to Exercise 2.2.v) with the best kernel (from Exercise 3.1.ii). Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)
```{python}
megmag_equal, y_equal = equalize_targets(megmag, pas_vector)
cv = StratifiedKFold()
svm_radial = SVC(kernel="rbf")

scores = []

for i in range(251):
    t = scaler.fit_transform(megmag_equal[:,:,i]);
    regr.fit(t, y_equal);
    temp_scores = cross_val_score(svm_radial, t, y_equal, cv=5);
    scores.append(np.mean(temp_scores))
```

```{python}
print("Highest classification accuracy is: %.3f" % np.amax(scores))
print("The timepoint at which this occurs is: %.f" % np.argmax(scores))
```
Chance level is 0.25, as there are four categories. The chance level is shown in the below graph as a green line.

```{python}
plt.figure() 
plt.plot(times, scores)
plt.axvline(times[226], color = "black")
plt.axhline(y = 0.25, color = "green")
plt.xlabel("Time (ms)")
plt.ylabel("Classification accuracy")
plt.title("Classification accuracy at given times")
plt.show()
```
    iv. Is classification of subjective experience possible at around 200-250 ms?
Yes. Classification scores are consistently above 0.25 during this time period, peaking around 0.35 and troughing at 0.27.
2) Finally, split the equalized data set (with all four ratings) into a training part and test part, where the test part if 30 % of the trials. Use `train_test_split` from `sklearn.model_selection`  
```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(megmag_equal, y_equal, test_size = 0.30)
```
    i. Use the kernel that resulted in the best classification in Exercise 3.1.ii and `fit`the training set and `predict` on the test set. This time your features are the number of sensors multiplied by the number of samples.  
```{python}
X_train = X_train.reshape(277, -1)
X_test = X_test.reshape(119, -1)
svm_radial.fit(X_train, y_train)
y_pred = svm_radial.predict(X_test)
```
    ii. Create a _confusion matrix_. It is a 4x4 matrix. The row names and the column names are the PAS-scores. There will thus be 16 entries. The PAS1xPAS1 entry will be the number of actual PAS1, $y_{pas1}$ that were predicted as PAS1, $\hat y_{pas1}$. The PAS1xPAS2 entry will be the number of actual PAS1, $y_{pas1}$ that were predicted as PAS2, $\hat y_{pas2}$ and so on for the remaining 14 entries.  Plot the matrix
```{python}

# Plotting using ConfusionMatrixDisplay
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(svm_radial, X_test, y_test) 
plt.xlabel("Predicted PAS-rating")
plt.ylabel("True PAS-rating")
plt.title("Confusion Matrix (119 predictions)")
plt.show()
```
    iii. Based on the confusion matrix, describe how ratings are misclassified and if that makes sense given that ratings should measure the strength/quality of the subjective experience. Is the classifier biased towards specific ratings?
Having run the classifier with different train/test splits it is clear that the classifier overestimates the presence of PAS 4. The results for PAS 1, 2 and 3 are different from time to time. Consistently, over 50 classifications for PAS 4 are predicted, which also drives the general accuracy: By predicting PAS 4 every time, the accuracy would be 0.25, as this is the number of PAS 4 in the data set. As such, the heavy bias towards PAS 4 means that this model is unsuitable for classification.
