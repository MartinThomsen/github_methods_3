df$correct <- as.numeric(df$correct)
m1_pooled <- glm(correct ~ target.frames, data = df, family = "binomial")
m1_partial <- glmer(correct ~ target.frames + (1|subject), data = df, family = "binomial")
likelihood_fun <- function(i) {
p <- fitted(i) # Vector of fitted values
y <- as.vector(model.response(model.frame(i), type = "numeric")) # Observed y-values
likelihood <- prod(p^y*(1-p)^(1-y)) # The likelihood function for logistic regression
return(likelihood)
}
log_likelihood_fun <- function(i) {
p <- fitted(i) # Vector of fitted values
y <- as.vector(model.response(model.frame(i), type = "numeric")) # Observed y-values
log_likelihood <- sum(y*log(p)+(1-y)*log(1-p)) # The log-likelihood function for logistic regression
return(log_likelihood)
}
likelihood_fun(m1_pooled)
log_likelihood_fun(m1_pooled)
logLik(m1_pooled)
log_likelihood_fun(m1_partial)
logLik(m1_partial)
m0 <- glm(correct ~ 1, data = df, family = 'binomial') # Null-model
m2 <- glmer(correct ~ 1 + (1|subject), data = df, family = 'binomial') # Null-model with subject intercepts
m1_partial # Model from before, predicted by target.frames and subject intercepts
m3 <- glmer(correct ~ target.frames + (target.frames|subject), data = df, family = "binomial")
anova(m2, m0, m1_partial, m3) # This function also give logLik values
anova(m1_partial, m3)
df %>% # Plot of group-level function per subject
ggplot() +
geom_smooth(aes(x = target.frames, y = fitted(m3), color = "m3")) +
geom_smooth(aes(x = target.frames, y = fitted(m1_pooled), color = "pooled model")) +
facet_wrap( ~ subject) +
labs(title = "Estimated group-level function pr. subject") +
labs(x = "target.frames", y = "fitted values (subject level)")+
theme_bw()
df_24 <- df %>%
filter(subject == "24")
t.test(df_24$correct, mu = 0.5)
m4 <- glmer(correct ~ target.frames + pas + (target.frames|subject), family = binomial, data = df)
m5 <- glmer(correct ~ target.frames * pas + (target.frames|subject), family = binomial, data = df, control = glmerControl(optimizer = "bobyqa"))
anova(m4, m5)
df %>%
ggplot()+
geom_smooth(aes(x = target.frames, y = fitted(m5), color = pas), method = "loess")+
theme_bw()
# Plotting using ggplot
ggplot(data = trial_df) +
geom_smooth(aes(x = target.frames, y = fitted(interaction_model), color = pas)) +
facet_wrap( ~ pas) +
xlim(0,8)
# Plotting using ggplot
ggplot(data = trial_df) +
geom_line(aes(x = target.frames, y = fitted(interaction_model), color = pas)) +
facet_wrap( ~ pas) +
xlim(0,8)
# Plotting using ggplot
ggplot(data = trial_df) +
geom_plot(aes(x = target.frames, y = fitted(interaction_model), color = pas)) +
facet_wrap( ~ pas) +
xlim(0,8)
# Plotting using ggplot
ggplot(data = trial_df) +
geom_point(aes(x = target.frames, y = fitted(interaction_model), color = pas)) +
facet_wrap( ~ pas) +
xlim(0,8)
summary(interaction_model)
# Already done, here is a summary of it.
summary(m5)
# testing whether PAS 2 is different from PAS 1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh_1 <- glht(interaction_model, contrast.vector)
print(summary(gh_1))
# Testing whether there is a difference in intercepts between PAS 2 and PAS 3
contrast.vector <- matrix(c(0, -1, 1, 0, 0, 0, 0, 0), nrow=1)
gh_2 <- glht(interaction_model, contrast.vector)
print(summary(gh_2))
# Testing whether rate of accuracy as a result of target frames increases for pas 2 over pas 1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh_1 <- glht(interaction_model, contrast.vector)
print(summary(gh_1))
# Testing whether rate of accuracy as a result of target frames increases for pas 3 over pas 2
contrast.vector <- matrix(c(0, 0, 0, 0, 0, -1, 1, 0), nrow=1)
gh_2 <- glht(m_target.pas.interact, contrast.vector)
print(summary(gh_2))
# Testing whether rate of accuracy as a result of target frames increases for pas 4 over pas 3
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), nrow=1)
gh_3 <- glht(m_target.pas.interact, contrast.vector)
print(summary(gh_3))
## testing whether PAS 2 is different from PAS 1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh_1 <- glht(m_target.pas.interact, contrast.vector)
print(summary(gh_1))
invlogit(coef(summary(gh_1))) # finding increase in percentile
## as another example, we could also test whether there is a difference in
## intercepts between PAS 2 and PAS 3
contrast.vector <- matrix(c(0, -1, 1, 0, 0, 0, 0, 0), nrow=1)
gh_2 <- glht(m_target.pas.interact, contrast.vector)
print(summary(gh_2))
# testing if accuracy performance increases faster for pas3 than pas2
contrast.vector <- matrix(c(0, 0, 0, 0, 0, -1, 1, 0), nrow=1)
gh_3 <- glht(m_target.pas.interact, contrast.vector)
print(summary(gh_3))
# testing if accuracy performance increases faster for pas4 than pas3
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), nrow=1)
gh_4 <- glht(m_target.pas.interact, contrast.vector)
print(summary(gh_4))
anova(gh_1, gh_3)
View(target_frame_model)
View(trial_df)
subject_7 = subset(trial_df, subject == "7")
subject_7 = c(subject_7$target.frames, subject_7$correct)
subject_7 = filter(trial_df, subject == "7") %>% select(target.frames, correct, pas)
subject_7 <- filter(trial_df, subject == "7") %>% select(target.frames, correct, pas)
subject_7 <- filter(trial_df, subject == "7") %>%
select(target.frames, correct, pas)
subject_7 <- trial_df %>%
filter(subject == "7") %>%
select(target.frames, correct, pas)
subject_7 <- trial_df %>%
filter(subject == "7") %>%
dplyr::select(target.frames, correct, pas)
View(subject_7)
load(dplyr)
install.packages('dplyr')
par = c(0.5,1,1,1)
subject7_pas1 <- optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
RSS <- function(dataset, par)
{
## "dataset" should be a data.frame containing the variables x (target.frames)
## and y (correct)
## "par" are our four parameters (a numeric vector)
par[1]=a
par[2]=b
par[3]=c
par[4]=d
x <- dataset$x
y <- dataset$y
y.hat <- a + ((b-a)/(1+exp(1)^((c-x)/d)))
RSS <- sum((y - y.hat)^2)
return(RSS)
}
subject7_pas1 <- optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
RSS <- function(dataset, par)
{
## "dataset" should be a data.frame containing the variables x (target.frames)
## and y (correct)
## "par" are our four parameters (a numeric vector)
a = par[1]
b = par[2]
c = par[3]
d = par[4]
x <- dataset$x
y <- dataset$y
y.hat <- a + ((b-a)/(1+exp(1)^((c-x)/d)))
RSS <- sum((y - y.hat)^2)
return(RSS)
}
subject7_pas1 <- optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas2 <- optim(data = filter(subject_7, pas == 2), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas3 <- optim(data = filter(subject_7, pas == 3), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas4 <- optim(data = filter(subject_7, pas == 4), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas1
subject7_pas1 <- optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas2 <- optim(data = filter(subject_7, pas == 2), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas3 <- optim(data = filter(subject_7, pas == 3), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas4 <- optim(data = filter(subject_7, pas == 4), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
print(subject7_pas1)
print(subject7_pas2)
print(subject7_pas3)
print(subject7_pas4)
df_007 <- df %>%
dplyr::filter(subject == "007") %>%
dplyr::select(target.frames, correct, pas) %>%
dplyr::rename(x = target.frames, y = correct)
par <- c(0.5, 1, 1, 1)
optim_007_pas1 <- optim(data = filter(df_007, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
optim_007_pas2 <- optim(data = filter(df_007, pas == 2), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
optim_007_pas3 <- optim(data = filter(df_007, pas == 3), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
optim_007_pas4 <- optim(data = filter(df_007, pas == 4), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
print(optim_7_pas1)
print(optim_007_pas1)
print(optim_007_pas2)
print(optim_007_pas3)
print(optim_007_pas4)
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, plyr, lme4, lmerTest, EnvStats, rstanarm, interactions)
pacman::p_load(multcomp) # for finding interactions in e.g.  2:3
list <- list.files(path = "data/experiment_1", pattern = "*.csv", full.names=TRUE) # importing all files in a list
df <- ldply(list, read_csv) # making them into one data-frame
df <- df %>%
filter(trial.type == "experiment")
df <- df %>%
mutate(correct = ifelse(target.type == "even" & obj.resp == "e" |
target.type == "odd" & obj.resp == "o", 1, 0))
df <- df %>%
mutate(subject = as.factor(subject)) %>%
mutate(task = as.factor(task)) %>%
mutate(cue = as.factor(cue)) %>%
mutate(pas = as.factor(pas)) %>%
mutate(trial = as.factor(trial)) %>%
mutate(trial.type = as.factor(trial.type)) %>%
mutate(target.frames = as.integer(target.frames))
unique(df$target.contrast)
unique(df$target.frames)
m_pool <- glm(correct ~ target.frames, data = df, family = binomial) # complete pooling model
m_partial_pool <- glmer(correct ~ target.frames + (1|subject), data = df, family = binomial) # partial pooling model
likelihood_function <- function(model, y) {
p <- fitted(model)
y <- y
return(prod(p^y*(1-p)^(1-y)))
}
log_likelihood_function <- function(model, y) {
p <- fitted(model)
y <- y
return(sum(y*log(p)+(1-y)*log(1-p)))
}
likelihood_function(m_pool, df$correct)
log_likelihood_function(m_pool, df$correct)
logLik(m_pool)
logLik(m_partial_pool)
log_likelihood_function(m_partial_pool, df$correct)
m_null <-          glm(correct ~ 1, family = binomial, data = df)
m_ran.int <-       glmer(correct ~ 1 + (1|subject), family = binomial, data = df)
m_target.int <-    glmer(correct ~ target.frames + (1|subject), family = binomial, data = df)
m_target.slope <-  glmer(correct ~ target.frames + (target.frames|subject), family = binomial, data = df)
m_target.slope.no.cor <- glmer(correct ~ target.frames + (target.frames||subject), family = binomial, data = df)
model <- c("m_null", "m_ran.int", "m_target.int", "m_target.slope.no.cor", "m_target.slope")
logLik_values <- anova(m_ran.int, m_null, m_target.int, m_target.slope, m_target.slope.no.cor)$logLik
cbind(model, logLik_values)
df %>%
ggplot() +
geom_smooth(aes(x = target.frames, y = fitted(m_target.slope), color = "Partial Pooling")) +
geom_smooth(aes(x = target.frames, y = fitted(m_pool), color = "Complete Pooling")) +
facet_wrap( ~ subject) +
labs(title = "Estimated Accuracy depended on Target Duration pr. subject") +
labs(x = "Stimulus duration (target.frames)", y = "Estimated accuracy")
df_024 <- df %>%
filter(subject == "024")
mean(df_024$correct)
t.test((df_024$correct), mu=0.5) # checking if the accuracy of participant 024 is significantly different from chance-level (50%)
m_target.pas <- glmer(correct ~ target.frames + pas + (target.frames|subject), family = binomial, data = df)
m_target.pas.interact <- glmer(correct ~ target.frames * pas + (target.frames|subject), family = binomial, data = df)
model <- c("m_target.int", "m_target.pas", "m_target.pas.interact")
logLik <- c(anova(m_target.int, m_target.pas, m_target.pas.interact)$logLik)
as.tibble(cbind(model, logLik))
# Plot showing accuracy dependend on target.duration and PAS pr. PAS
df %>%
ggplot() +
geom_point(aes(x = target.frames, y = fitted(m_target.pas.interact), color = pas))+
facet_wrap( ~ pas) +
labs(title = "Estimated accurary dependent on target duration and PAS") +
labs(x = "Target duration (target.frames)", y = "Estimated Accuracy") +
theme_bw()
# Similar plot but more intuitive
interactions::interact_plot(model = m_target.pas.interact, pred = "target.frames", modx = "pas") # visualizing the effects of pas and targetframes and their interactions
# creating a table showing the percentile increase in accuracy for every fixed effect and interactions
estimates <- c(coef(summary(m_target.pas.interact))[1:8])
accuracy_increase_in_prob <- c(invlogit(estimates))
estimates <- c("intercept", "target.frames", "pas2", "pas3", "pas4", "target.frames:pas2", "target.frames:pas3", "target.frames:pas4")
as.tibble(cbind(estimates, accuracy_increase_in_prob))
RSS <- function(dataset, par)
{
## "dataset" should be a data.frame containing the variables x (target.frames)
## and y (correct)
## "par" are our four parameters (a numeric vector)
a = par[1]
b = par[2]
c = par[3]
d = par[4]
x <- dataset$x
y <- dataset$y
y.hat <- a + ((b-a)/(1+exp(1)^((c-x)/d)))
RSS <- sum((y - y.hat)^2)
return(RSS)
}
df_007 <- df %>%
dplyr::filter(subject == "007") %>%
dplyr::select(target.frames, correct, pas) %>%
dplyr::rename(x = target.frames, y = correct)
par <- c(0.5, 1, 1, 1)
optim_007_pas1 <- optim(data = filter(df_007, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
optim_007_pas2 <- optim(data = filter(df_007, pas == 2), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
optim_007_pas3 <- optim(data = filter(df_007, pas == 3), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
optim_007_pas4 <- optim(data = filter(df_007, pas == 4), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
View(optim_007_pas1)
View(subject7_pas1)
View(optim_007_pas1)
View(optim_007_pas2)
View(optim_007_pas3)
View(optim_007_pas4)
View(optim_007_pas3)
RSS(subject7_pas1)
RSS(subject7_pas1,par)
RSS(optim_007_pas1, par)
RSS(subject_7, par)
?optim
View(subject_7)
RSS <- function(dataset, par)
{
## "dataset" should be a data.frame containing the variables x (target.frames)
## and y (correct)
## "par" are our four parameters (a numeric vector)
a = par[1]
b = par[2]
c = par[3]
d = par[4]
x <- dataset$x
y <- dataset$y
y.hat <- a + ((b-a)/(1+exp(1)^((c-x)/d)))
RSS <- sum((y - y.hat)^2)
return(RSS)
}
subject7_pas1 = optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas2 = optim(data = filter(subject_7, pas == 2), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas3 = optim(data = filter(subject_7, pas == 3), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas4 = optim(data = filter(subject_7, pas == 4), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas1
RSS <- function(dataset, par)
{
## "dataset" should be a data.frame containing the variables x (target.frames)
## and y (correct)
## "par" are our four parameters (a numeric vector)
a = par[1]
b = par[2]
c = par[3]
d = par[4]
x <- dataset$target.frames
y <- dataset$correct
y.hat <- a + ((b-a)/(1+exp(1)^((c-x)/d)))
RSS <- sum((y - y.hat)^2)
return(RSS)
}
subject7_pas1 = optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas2 = optim(data = filter(subject_7, pas == 2), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas3 = optim(data = filter(subject_7, pas == 3), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas4 = optim(data = filter(subject_7, pas == 4), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas1
subject7_pas2
subject7_pas3
RSS(subject_7,par)
cbind(subject7_pas1,subject7_pas2,subject7_pas3,subject7_pas4)
cbind(subject7_pas1$par,subject7_pas2$par,subject7_pas3$par,subject7_pas4$par)
0.52+0.794+0.13
/4
1.444/4
par_values = cbind(subject7_pas1$par,subject7_pas2$par,subject7_pas3$par,subject7_pas4$par)
sum(par_values[2])
sum(par_values[2,:])
sum(par_values[2,;])
cbind(subject7_pas1$par,subject7_pas2$par,subject7_pas3$par,subject7_pas4$par)
sum(par_values(:,1))
par_values = cbind(subject7_pas1$par,subject7_pas2$par,subject7_pas3$par,subject7_pas4$par)
cbind(subject7_pas1$par,subject7_pas2$par,subject7_pas3$par,subject7_pas4$par)
par_values[1,1]
par_values[1,2]
sum(par_values[2,])
0.533+0.61+2.0+0.06
0.523+0.61+0.93+0.99
a = mean(par_values[,1])
a
a = mean(par_values[,1])
b = mean(par_values[,2])
c = mean(par_values[,3])
d = mean(par_values[,4])
a,b,c,d
par_values = cbind(subject7_pas1$par,subject7_pas2$par,subject7_pas3$par,subject7_pas4$par)
a = mean(par_values[,1])
b = mean(par_values[,2])
c = mean(par_values[,3])
d = mean(par_values[,4])
a,b,c,d
print("a =" mean(par_values[,1]))
print("a=" a)
print("a=", a)
print("a={}", a)
par_values = cbind(subject7_pas1$par,subject7_pas2$par,subject7_pas3$par,subject7_pas4$par)
a = mean(par_values[,1])
b = mean(par_values[,2])
c = mean(par_values[,3])
d = mean(par_values[,4])
a
b
c
d
par_values
optim_007_pas1
cbind(optim_007_pas1,optim_007_pas2,optim_007_pas3,optim_007_pas4)
jj = cbind(optim_007_pas1,optim_007_pas2,optim_007_pas3,optim_007_pas4)
jj$par
cbind(optim_007_pas1$par,optim_007_pas2$par,optim_007_pas3$par,optim_007_pas4$par)
par_values
mean(par_values[1,])
mean(par_values[2,])
mean(par_values[3,])
mean(par_values[4,])
knitr::opts_chunk$set(echo = TRUE)
politeness <- read.csv('politeness.csv')
RSS <- function(dataset, par)
{
## "dataset" should be a data.frame containing the variables x (target.frames)
## and y (correct)
## "par" are our four parameters (a numeric vector)
a = par[1]
b = par[2]
c = par[3]
d = par[4]
x <- dataset$target.frames
y <- dataset$correct
y.hat <- a + ((b-a)/(1+exp(1)^((c-x)/d)))
RSS <- sum((y - y.hat)^2)
return(RSS)
}
subject7_pas1 = optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, lmerTest, lme4, gridExtra, dfoptim, readbulk, boot, multcomp)
experiment_df = read_bulk("experiment_1")
experiment_df$trial.type = factor(experiment_df$trial.type)
experiment_df$cue = factor(experiment_df$cue)
experiment_df$task = factor(experiment_df$task)
experiment_df$pas = factor(experiment_df$pas)
experiment_df$subject = factor(experiment_df$subject)
trial_df <- subset(experiment_df, experiment_df$trial.type == "experiment")
# Create variable and set all to 0
trial_df$correct = 0
# Loop through data and change "correct" to 1 where response matches target type
for(i in 1:length(trial_df$trial.type))
{
if (trial_df$obj.resp[i]== 'o' && trial_df$target.type[i] == 'odd')
{
trial_df$correct[i] = 1
}
else if(trial_df$obj.resp[i]== 'e' && trial_df$target.type[i] == 'even'){
trial_df$correct[i] = 1
}
}
pooled_model = glm(correct ~ target.frames, data = trial_df, family = binomial)
partial_model = glmer(correct ~ target.frames + (1|subject), data = trial_df, family = binomial)
# Creating a likelihood function, which takes a model and a vector as input
likelihood_fun <- function(model, y)
{
p <- fitted(model) # extract fitted values
return(prod(p^y*(1-p)^(1-y)))
}
# Creating a logarithmic likelihood function, which takes a model and a vector as input
log_likelihood_fun <- function(model, y)
{
p <- fitted(model) # extract fitted values
return(sum(y*log(p)+(1-y)*log(1-p)))
}
likelihood_fun(pooled_model, trial_df$correct)
log_likelihood_fun(pooled_model, trial_df$correct)
logLik(pooled_model)
log_likelihood_fun(partial_model, trial_df$correct)
logLik(partial_model)
# Creating models
null_model = glm(correct ~ 1, family = binomial, data = trial_df) # Null model
subject_model = glmer(correct ~ 1 +(1|subject), family = binomial, data = trial_df) # Added subject-level intercept ((1|subject))
target_frame_model = glmer(correct ~ target.frames + (1|subject), family = binomial, data = trial_df) # Target.frames added as group-level effect (target.frames)
slope_model = glmer(correct ~ target.frames + (target.frames|subject), family = binomial, data = trial_df) # Subject-level slopes for target.frames added (target.frames|subject)
correlation_model = glmer(correct ~ target.frames + (target.frames||subject), family = binomial, data = trial_df) # Honestly not sure if this is correct, I was having a difficult time finding documentation online
# Combining model names in order to provide a prettier output to reader
model = c("null_model", "subject_model", "target_frame_model", "slope_model", "correlation_model")
log_likelihood = c(logLik(null_model), logLik(subject_model), logLik(target_frame_model), logLik(slope_model), logLik(correlation_model))
# Display as combined data
cbind(model, log_likelihood)
anova(slope_model, correlation_model)
# Plotting using ggplot
ggplot(data = trial_df) +
geom_point(aes(x = target.frames, y = fitted(pooled_model), color = "Pooled model")) +
geom_point(aes(x = target.frames, y = fitted(slope_model), color = "Added slope")) +
facet_wrap( ~ subject) +
xlim(0,8)
subject12_df = subset(trial_df, subject == "12")
subject24_df = subset(trial_df, subject == "24")
t.test(subject12_df$correct, mu = 0.5)
t.test(subject24_df$correct, mu = 0.5)
pas_model = glmer(correct ~ target.frames + pas + (target.frames|subject), family = binomial, data = trial_df)
log_likelihood_fun(pas_model, trial_df$correct)
interaction_model = glmer(correct ~ target.frames * pas + (target.frames|subject), family = binomial, data = trial_df)
logLik(interaction_model)
interaction_model = glmer(correct ~ target.frames * pas + (target.frames|subject), family = binomial, data = trial_df, control = glmerControl(optimizer="bobyqa"))
logLik(interaction_model)
anova(slope_model, pas_model, interaction_model)
# Plotting using ggplot
ggplot(data = trial_df) +
geom_point(aes(x = target.frames, y = fitted(interaction_model), color = pas)) +
facet_wrap( ~ pas) +
xlim(0,8)
summary(interaction_model)
subject_7 <- trial_df %>%
filter(subject == "7") %>%
dplyr::select(target.frames, correct, pas)
par = c(0.5,1,1,1)
subject7_pas1 = optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
RSS <- function(dataset, par)
{
## "dataset" should be a data.frame containing the variables x (target.frames)
## and y (correct)
## "par" are our four parameters (a numeric vector)
a = par[1]
b = par[2]
c = par[3]
d = par[4]
x <- dataset$target.frames
y <- dataset$correct
y.hat <- a + ((b-a)/(1+exp(1)^((c-x)/d)))
RSS <- sum((y - y.hat)^2)
return(RSS)
}
subject7_pas1 = optim(data = filter(subject_7, pas == 1), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas2 = optim(data = filter(subject_7, pas == 2), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas3 = optim(data = filter(subject_7, pas == 3), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
subject7_pas4 = optim(data = filter(subject_7, pas == 4), fn = RSS, par = par, method = 'L-BFGS-B', lower = c(0, 0, -Inf, -Inf), upper = c(1, 1, Inf, Inf))
